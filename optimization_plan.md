# 医学文本分割器优化方案

## 1. 简介
本文档概述了一个优化 `medical_splitter.py` 中文本分割策略的方案，以在使用中文医学教材时获得更好的检索结果。

## 2. 当前策略分析
当前 `medical_splitter.py` 中的策略具有以下特点：
- **自适应块大小：** 根据 `term_density`（医学术语密度）动态调整 `chunk_size` 和 `chunk_overlap`。 较高密度术语文本的块较小，重叠更多。
- **基于规则的分割：** 使用 `RecursiveCharacterTextSplitter`，分隔符包括段落、换行符、句号、分号、逗号和空格。
- **医学术语识别：** 使用正则表达式识别医学术语、剂量/值和缩写。
- **块后处理：** 清理块以确保它们以句点、感叹号或问号结尾。

## 3. 潜在的优化点
- **中文分词：** 当前的分词器依赖于空格和标点符号进行分割，这对于中文来说是不够的。 医学教科书包含大量专业术语，不正确的分词会影响检索结果。 考虑集成中文分词工具（如 `jieba`）并添加医学词典。
- **章节结构保留：** 章节、节和小节的正则表达式在代码中定义，但在分割期间不使用此信息。 可以修改分割逻辑以尝试保留章节标题和分层结构，以确保上下文完整性。
- **块大小和重叠：** 当前的 `chunk_size` 和 `chunk_overlap` 是根据 `term_density` 计算的固定值。 可以根据教科书的特点调整这些值，或者引入更精细的自适应策略。
- **特殊医学格式处理：** 可以处理特殊的医学格式，例如药物名称、剂量和适应症，例如，使用正则表达式提取此信息并将其作为元数据添加到文档中。
- **语义分割：** 基于规则的分割可能无法完全保证语义完整性。 考虑引入语义分割算法，例如使用句子嵌入来计算相似度，以避免在语义上不相关的句子之间进行分割。

## 4. 优化方案
1. **引入 `jieba` 分词：** 修改 `split_document` 方法以使用 `jieba` 分割文本并根据分割结果进行分割。
2. **章节结构保留：** 修改 `split_document` 方法以在分割前识别章节标题，并尝试将章节内容保留在一个块中。
3. **调整块大小和重叠：** 根据教科书的特点，将 `chunk_size` 调整为 800-1200 个字符，并将 `chunk_overlap` 调整为 15-20%。
4. **添加医学词典：** 将医学词典添加到 `jieba` 分词器以提高医学术语分割的准确性。
5. **后处理：** 添加后处理步骤以合并太短的块以避免碎片化。

## 5. 实施细则
（在实施期间填写）

## 6. 测试与评估
（在测试期间填写）